{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/valid_df.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "arg_id          3458\nkey_point_id    3458\nlabel           3458\nargument        3458\ntopic           3458\nstance          3458\nkey_point       3458\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "entailment_model = \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(entailment_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(entailment_model).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: school uniforms cut down on bulling and keep everyone the same .\n",
      "Premise: School uniform reduces bullying\n",
      "\n",
      "True entailment score: 0.9782942533493042\n",
      "\n",
      "Dropping word 1 \"School\" in premise.\n",
      "Entailment score is: 0.9620389342308044\n",
      "That's a difference of 0.016255319118499756.\n",
      "\n",
      "Dropping word 2 \"uniform\" in premise.\n",
      "Entailment score is: 0.9140735864639282\n",
      "That's a difference of 0.06422066688537598.\n",
      "\n",
      "Dropping word 3 \"reduces\" in premise.\n",
      "Entailment score is: 0.05332595482468605\n",
      "That's a difference of 0.9249682985246181.\n",
      "\n",
      "Dropping word 4 \"bullying\" in premise.\n",
      "Entailment score is: 0.834721565246582\n",
      "That's a difference of 0.14357268810272217.\n",
      "\n",
      "Dropping word 0 \".\" in hypothesis.\n",
      "Entailment score is: 0.9770272970199585\n",
      "That's a difference of 0.0012669563293457031.\n",
      "\n",
      "Dropping word 6 \"school\" in hypothesis.\n",
      "Entailment score is: 0.19731928408145905\n",
      "That's a difference of 0.7809749692678452.\n",
      "\n",
      "Dropping word 2 \"uniforms\" in hypothesis.\n",
      "Entailment score is: 0.07608986645936966\n",
      "That's a difference of 0.9022043868899345.\n",
      "\n",
      "Dropping word 3 \"cut\" in hypothesis.\n",
      "Entailment score is: 0.9516233205795288\n",
      "That's a difference of 0.02667093276977539.\n",
      "\n",
      "Dropping word 4 \"down\" in hypothesis.\n",
      "Entailment score is: 0.9602919220924377\n",
      "That's a difference of 0.018002331256866455.\n",
      "\n",
      "Dropping word 5 \"on\" in hypothesis.\n",
      "Entailment score is: 0.9826689958572388\n",
      "That's a difference of -0.00437474250793457.\n",
      "\n",
      "Dropping word 6 \"bulling\" in hypothesis.\n",
      "Entailment score is: 0.02666369266808033\n",
      "That's a difference of 0.9516305606812239.\n",
      "\n",
      "Dropping word 7 \"and\" in hypothesis.\n",
      "Entailment score is: 0.9732165336608887\n",
      "That's a difference of 0.005077719688415527.\n",
      "\n",
      "Dropping word 8 \"keep\" in hypothesis.\n",
      "Entailment score is: 0.9686806797981262\n",
      "That's a difference of 0.009613573551177979.\n",
      "\n",
      "Dropping word 9 \"everyone\" in hypothesis.\n",
      "Entailment score is: 0.9641989469528198\n",
      "That's a difference of 0.014095306396484375.\n",
      "\n",
      "Dropping word 10 \"the\" in hypothesis.\n",
      "Entailment score is: 0.9739236831665039\n",
      "That's a difference of 0.004370570182800293.\n",
      "\n",
      "Dropping word 11 \"same\" in hypothesis.\n",
      "Entailment score is: 0.974371075630188\n",
      "That's a difference of 0.003923177719116211.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "#### THIS IS THE LEAVE ONE OUT CELL#########\n",
    "############################################\n",
    "\n",
    "MAX_LENGTH=256\n",
    "import copy\n",
    "\n",
    "def leave_one_out(hypothesis, premise):\n",
    "\n",
    "    print(f\"Hypothesis: {hypothesis}\")\n",
    "    print(f\"Premise: {premise}\\n\")\n",
    "\n",
    "\n",
    "    n = len(premise.split()) + len(hypothesis.split())\n",
    "\n",
    "    true_score = compute_entailment(hypothesis, premise)[\"entail\"]\n",
    "    print(f\"True entailment score: {true_score}\\n\")\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        premise_copy = copy.copy(premise).split()\n",
    "        hypothesis_copy = copy.copy(hypothesis).split()\n",
    "\n",
    "        # Drop word in the respective sequence\n",
    "        index = None\n",
    "        if i < len(premise_copy):\n",
    "            dropped_word = premise_copy.pop(i)\n",
    "            which = \"premise\"\n",
    "        else:\n",
    "            index = i - len(premise_copy) - 1\n",
    "            dropped_word = hypothesis_copy.pop(index)\n",
    "            which = \"hypothesis\"\n",
    "\n",
    "        premise_copy = \" \".join(premise_copy)\n",
    "        hypothesis_copy = \" \".join(hypothesis_copy)\n",
    "\n",
    "        score = compute_entailment(hypothesis_copy, premise_copy)[\"entail\"]\n",
    "\n",
    "        print(f\"Dropping word {index+1 if index else i+1} \\\"{dropped_word}\\\" in {which}.\")\n",
    "        print(f\"Entailment score is: {score}\")\n",
    "        print(f\"That's a difference of {true_score-score}.\\n\")\n",
    "\n",
    "def compute_entailment(hypothesis, premise):\n",
    "    tokenized_input_seq_pair = tokenizer.encode_plus(hypothesis, premise, max_length=MAX_LENGTH, return_token_type_ids=True, truncation=True)\n",
    "    input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0).cuda()\n",
    "    token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0).cuda()\n",
    "    attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0).cuda()\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=None)\n",
    "    predicted_probability = torch.softmax(outputs[0], dim=1)[0].tolist()\n",
    "    entailment_prob = predicted_probability[0]\n",
    "    neutral_prob = predicted_probability[1]\n",
    "    contradiction_prob = predicted_probability[2]\n",
    "    result = {'entail':entailment_prob, 'neutral':neutral_prob, 'contradict':contradiction_prob}\n",
    "    return result\n",
    "\n",
    "\n",
    "arg = \"\"\"school uniforms cut down on bulling and keep everyone the same .\"\"\"\n",
    "kp = \"\"\"School uniform reduces bullying\"\"\"\n",
    "\n",
    "arg_list = arg.split()\n",
    "kp_list = kp.split()\n",
    "\n",
    "leave_one_out(arg, kp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entail': 0.9782942533493042, 'neutral': 0.020531466230750084, 'contradict': 0.0011742558563128114}\n",
      "{'entail': 0.8369314074516296, 'neutral': 0.15482930839061737, 'contradict': 0.008239319548010826}\n"
     ]
    }
   ],
   "source": [
    "print(compute_entailment_hyp_prem(arg, kp))\n",
    "print(compute_entailment_sentence(arg+\" \"+kp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "'school uniforms cut down on bulling and keep everyone the same. School uniform reduces bullying'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg+\" \"+kp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_argument_with_keypoints(result, kp_dict, arg_dict):\n",
    "    for arg_id, arg in arg_dict.items():\n",
    "        result[arg_id] = {}\n",
    "        for kp_id, kp in kp_dict.items():\n",
    "            result[arg_id][kp_id] = compute_entailment(arg, kp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11322252f37641afbbd5fb0ef0932c50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_282737/2106863543.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mtopic_arg_dict\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtopic_argument_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtopic_arguments\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0;31m# match\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0margument_keypoints\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmatch_argument_with_keypoints\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margument_keypoints\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtopic_kp_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtopic_arg_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_282737/24083671.py\u001B[0m in \u001B[0;36mmatch_argument_with_keypoints\u001B[0;34m(result, kp_dict, arg_dict)\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkp_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkp_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m             \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0marg_id\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkp_id\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_entailment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_282737/24083671.py\u001B[0m in \u001B[0;36mmatch_argument_with_keypoints\u001B[0;34m(result, kp_dict, arg_dict)\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mkp_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkp\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkp_dict\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m             \u001B[0mresult\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0marg_id\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkp_id\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompute_entailment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "argument_keypoints = {}\n",
    "for topic in tqdm(valid_df.topic.unique()):\n",
    "    for stance in [-1, 1]:\n",
    "        topic_keypoint_ids = valid_df[(valid_df.topic==topic) & (valid_df.stance==stance)]['key_point_id'].tolist()\n",
    "        topic_keypoints = valid_df[(valid_df.topic==topic) & (valid_df.stance==stance)]['key_point'].tolist()\n",
    "        topic_kp_dict = dict(zip(topic_keypoint_ids, topic_keypoints))\n",
    "        \n",
    "        topic_argument_ids = valid_df[(valid_df.topic==topic) & (valid_df.stance==stance)]['arg_id'].tolist()\n",
    "        topic_arguments = valid_df[(valid_df.topic==topic) & (valid_df.stance==stance)]['argument'].tolist()\n",
    "        topic_arg_dict= dict(zip(topic_argument_ids, topic_arguments))\n",
    "        # match \n",
    "        argument_keypoints = match_argument_with_keypoints(argument_keypoints, topic_kp_dict, topic_arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(argument_keypoints, open(\"entailment_all_valid_predictions.json\", \"w\",encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "loaded predictions for 932 arguments\r\n",
      "mAP strict= 0.6896999511088654 ; mAP relaxed = 0.8973558780369991\r\n"
     ]
    }
   ],
   "source": [
    "! python3 ../src-py/track_1_kp_matching.py ../data entailment_all_valid_predictions.json our_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}